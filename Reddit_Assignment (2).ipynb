{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5fLxAoNv1vnx",
        "outputId": "ae417059-3b95-4dc2-e4a1-05eb65f4c60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->openai) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw openai requests python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import openai\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "from google.colab import files\n",
        "import requests\n",
        "\n",
        "# --- Credentials ---\n",
        "REDDIT_CLIENT_ID = \"\"\n",
        "REDDIT_CLIENT_SECRET = \"\"\n",
        "REDDIT_USER_AGENT = \"\"\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "REDDIT_USERNAME = \"\"\n",
        "REDDIT_PASSWORD = \"\"\n",
        "\n",
        "# --- OpenRouter Setup (for openai v0.28.0) ---\n",
        "openai.api_key = OPENROUTER_API_KEY\n",
        "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "print(\" Setup complete! Ready to analyze Reddit users.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojfb3c922lxa",
        "outputId": "7a2a769f-2096-49a6-bd4c-a1f7066b6ce1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Setup complete! Ready to analyze Reddit users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RedditPersonaAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the Reddit API client\"\"\"\n",
        "        self.reddit = praw.Reddit(\n",
        "            client_id=REDDIT_CLIENT_ID,\n",
        "            client_secret=REDDIT_CLIENT_SECRET,\n",
        "            user_agent=REDDIT_USER_AGENT,\n",
        "            username=REDDIT_USERNAME,\n",
        "            password=REDDIT_PASSWORD\n",
        "        )\n",
        "        # Configuration\n",
        "        self.max_posts = 10\n",
        "        self.max_comments = 20\n",
        "\n",
        "    def extract_username_from_url(self, url: str) -> str:\n",
        "        \"\"\"Extract username from Reddit profile URL\"\"\"\n",
        "        url = url.rstrip('/')\n",
        "        match = re.search(r'/user/([^/]+)', url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid Reddit URL format: {url}\")\n",
        "\n",
        "    def scrape_user_data(self, username: str) -> Dict:\n",
        "        \"\"\"Scrape posts and comments from a Reddit user\"\"\"\n",
        "        try:\n",
        "            user = self.reddit.redditor(username)\n",
        "            # This check will raise an exception if the user doesn't exist\n",
        "            user.id\n",
        "\n",
        "            posts_data = []\n",
        "            comments_data = []\n",
        "            print(f\"Scraping data for user: {username}\")\n",
        "\n",
        "            # Scrape posts\n",
        "            print(\" Collecting posts...\")\n",
        "            for post in user.submissions.new(limit=self.max_posts):\n",
        "                posts_data.append({\n",
        "                    'url': f\"https://reddit.com{post.permalink}\",\n",
        "                    'subreddit': str(post.subreddit),\n",
        "                    'title': post.title,\n",
        "                    'selftext': post.selftext\n",
        "                })\n",
        "\n",
        "            # Scrape comments\n",
        "            print(\"Collecting comments...\")\n",
        "            for comment in user.comments.new(limit=self.max_comments):\n",
        "                comments_data.append({\n",
        "                    'url': f\"https://reddit.com{comment.permalink}\",\n",
        "                    'subreddit': str(comment.subreddit),\n",
        "                    'body': comment.body\n",
        "                })\n",
        "\n",
        "            print(f\"Scraped {len(posts_data)} posts and {len(comments_data)} comments\")\n",
        "            return {\n",
        "                'username': username,\n",
        "                'posts': posts_data,\n",
        "                'comments': comments_data\n",
        "            }\n",
        "        except Exception as e:\n",
        "            # Catch PRAW exceptions for non-existent users\n",
        "            raise Exception(f\"User {username} not found or suspended. Original error: {e}\")\n",
        "\n",
        "    def generate_persona_with_citations(self, user_data: Dict) -> str:\n",
        "        \"\"\"Generate user persona using OpenRouter with citations\"\"\"\n",
        "        # Prepare a simplified list of content for the prompt\n",
        "        content_for_prompt = \"\"\n",
        "        for post in user_data['posts']:\n",
        "            content_for_prompt += f\"Post in r/{post['subreddit']} with title '{post['title']}': {post['selftext']} [citation: {post['url']}]\\\\n\"\n",
        "        for comment in user_data['comments']:\n",
        "            content_for_prompt += f\"Comment in r/{comment['subreddit']} says: {comment['body']} [citation: {comment['url']}]\\\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the following Reddit activity to create a user persona. For each characteristic (like interests, personality traits, etc.), you MUST provide the specific citation URL given in the text.\n",
        "\n",
        "        User Activity:\n",
        "        {content_for_prompt}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # CORRECTED: Use the syntax for openai v0.28.0\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"anthropic/claude-3.5-sonnet\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at creating user personas from text. You must always cite your sources using the URLs provided.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=1000,\n",
        "                headers={ \"HTTP-Referer\": \"http://localhost\", \"X-Title\": \"Reddit Persona Bot\" }\n",
        "            )\n",
        "            return response['choices'][0]['message']['content']\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error generating persona: {str(e)}\")\n",
        "\n",
        "    def save_persona_to_file(self, persona_text: str, username: str) -> str:\n",
        "        \"\"\"Save the generated persona to a text file\"\"\"\n",
        "        filename = f\"{username}_persona.txt\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(persona_text)\n",
        "        print(f\" Persona saved to: {filename}\")\n",
        "        return filename\n",
        "\n",
        "    def analyze_user(self, profile_url: str) -> str:\n",
        "        \"\"\"Main method to analyze a Reddit user and generate persona\"\"\"\n",
        "        try:\n",
        "            username = self.extract_username_from_url(profile_url)\n",
        "            print(f\" Analyzing user: {username}\")\n",
        "            user_data = self.scrape_user_data(username)\n",
        "            if not user_data['posts'] and not user_data['comments']:\n",
        "                print(f\"No activity found for user {username}.\")\n",
        "                return None\n",
        "            print(\" Generating persona with AI...\")\n",
        "            persona_text = self.generate_persona_with_citations(user_data)\n",
        "            filename = self.save_persona_to_file(persona_text, username)\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            print(f\" Error during analysis: {str(e)}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "FiaRvq2N2vfw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = RedditPersonaAnalyzer()\n",
        "print(\" Reddit Persona Analyzer initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URaWYsHJ47p5",
        "outputId": "b2fec490-dc14-4841-baf4-0901021b0edd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Reddit Persona Analyzer initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "working_sample_users = [\n",
        "    \"https://www.reddit.com/user/kojied/comments/\",\n",
        "    \"https://www.reddit.com/user/Hungry-Move-6603/comments/\"\n",
        "]\n",
        "\n",
        "print(\" Testing with working sample users...\")\n",
        "print(\"Note: Some users from previous tests might be suspended/deleted\")\n",
        "\n",
        "for url in working_sample_users:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing: {url}\")\n",
        "    print('='*60)\n",
        "\n",
        "    try:\n",
        "        result = analyzer.analyze_user(url)\n",
        "\n",
        "        if result:\n",
        "            print(f\"✅ Success! Results saved to: {result}\")\n",
        "\n",
        "            # Display first few lines of the result\n",
        "            try:\n",
        "                with open(result, 'r', encoding='utf-8') as f:\n",
        "                    lines = f.readlines()\n",
        "                    print(\"\\n Preview of generated persona:\")\n",
        "                    for i, line in enumerate(lines[:15]):\n",
        "                        print(line.strip())\n",
        "                    if len(lines) > 15:\n",
        "                        print(\"... (truncated)\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not preview file: {e}\")\n",
        "        else:\n",
        "            print(\"Failed to analyze this user\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error during analysis: {str(e)}\")\n",
        "\n",
        "    print(\"\\n⏳ Waiting 3 seconds before next user...\")\n",
        "    time.sleep(3)  # Longer pause between users to avoid rate limiting\n",
        "\n",
        "print(\"\\n🎉 Testing complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYAuokxRfmsU",
        "outputId": "9f842ce6-23af-4ef8-993d-db26460aa34b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Testing with working sample users...\n",
            "Note: Some users from previous tests might be suspended/deleted\n",
            "\n",
            "============================================================\n",
            "Testing: https://www.reddit.com/user/kojied/comments/\n",
            "============================================================\n",
            " Analyzing user: kojied\n",
            "Scraping data for user: kojied\n",
            " Collecting posts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comments...\n",
            "Scraped 10 posts and 20 comments\n",
            " Generating persona with AI...\n",
            " Persona saved to: kojied_persona.txt\n",
            "✅ Success! Results saved to: kojied_persona.txt\n",
            "\n",
            " Preview of generated persona:\n",
            "Based on the provided Reddit activity, I'll create a detailed user persona:\n",
            "\n",
            "Demographics & Location:\n",
            "- Lives in New York City (relatively new resident, ~3 years) [https://reddit.com/r/newyorkcity/comments/1lykkqf/i_feel_violated_by_intern_season/]\n",
            "- Millennial age group [https://reddit.com/r/AskReddit/comments/1kz5a0n/do_you_think_millennials_generally_look_younger/mv2xf3q/]\n",
            "- Japanese language speaker/cultural knowledge [https://reddit.com/r/OnePiece/comments/1kpavbs/ive_been_ripping_on_that_part_of_zoros_backstory/mswlma6/]\n",
            "\n",
            "Professional Background:\n",
            "- Works in technology, specifically iOS/VR development [https://reddit.com/r/visionosdev/comments/1b3yugb/best_blogs_tutorial_channels_to_learn/]\n",
            "- Shows interest in H1B visa topics, suggesting possible personal connection to immigration [https://reddit.com/r/AskReddit/comments/1hnx8j0/h1b_holders_what_are_your_thoughts_on_the/]\n",
            "\n",
            "Interests:\n",
            "1. Technology:\n",
            "- Particularly interested in VR/AR and spatial computing [https://reddit.com/r/VisionPro/comments/1b4yi15/spacial_tours_with_3d_map_and_360_video/]\n",
            "- Follows AI/ChatGPT developments [https://reddit.com/r/ChatGPT/comments/1lvtwj3/i_used_ai_to_create_this_short_film_on_human/n2b07h0/]\n",
            "... (truncated)\n",
            "\n",
            "⏳ Waiting 3 seconds before next user...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Testing: https://www.reddit.com/user/Hungry-Move-6603/comments/\n",
            "============================================================\n",
            " Analyzing user: Hungry-Move-6603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping data for user: Hungry-Move-6603\n",
            " Collecting posts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comments...\n",
            "Scraped 3 posts and 12 comments\n",
            " Generating persona with AI...\n",
            " Persona saved to: Hungry-Move-6603_persona.txt\n",
            "✅ Success! Results saved to: Hungry-Move-6603_persona.txt\n",
            "\n",
            " Preview of generated persona:\n",
            "Based on the provided Reddit activity, I'll create a user persona with specific citations:\n",
            "\n",
            "Name: Unspecified\n",
            "Location History:\n",
            "- Originally from Delhi, recently moved to Lucknow in December 2024 for business purposes\n",
            "[Citation: https://reddit.com/r/lucknow/comments/1lwyhny/everyone_is_something_in_lko/]\n",
            "\n",
            "Professional Background:\n",
            "- Business-oriented individual\n",
            "[Citation: https://reddit.com/r/lucknow/comments/1lwyhny/everyone_is_something_in_lko/]\n",
            "\n",
            "Interests & Lifestyle:\n",
            "1. Reading enthusiast (seeking reading clubs/cafes)\n",
            "[Citation: https://reddit.com/r/lucknow/comments/1lzuq0r/reading_cafe_reader_club/]\n",
            "\n",
            "... (truncated)\n",
            "\n",
            "⏳ Waiting 3 seconds before next user...\n",
            "\n",
            "🎉 Testing complete!\n"
          ]
        }
      ]
    }
  ]
}